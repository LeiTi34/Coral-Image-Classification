{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras import mixed_precision\n",
    "import tensorflow as tf\n",
    "\n",
    "# Enable mixed precision\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_global_policy(policy)\n",
    "\n",
    "# Limit GPU memory growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "# Dataset Overview\n",
    "bleached_path = \"./data/bleached_corals\"\n",
    "healthy_path = \"./data/healthy_corals\"\n",
    "\n",
    "def get_image_paths(folder_path):\n",
    "    return [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.jpg')]\n",
    "\n",
    "# Get all image paths\n",
    "bleached_image_paths = get_image_paths(bleached_path)\n",
    "healthy_image_paths = get_image_paths(healthy_path)\n",
    "\n",
    "def load_and_resize_images(image_paths, size=(224, 224)):\n",
    "    images = []\n",
    "    for img_path in image_paths:\n",
    "        with Image.open(img_path) as img:\n",
    "            img = img.resize(size)\n",
    "            images.append(np.array(img))\n",
    "    return np.array(images)\n",
    "\n",
    "bleached_images = load_and_resize_images(bleached_image_paths)\n",
    "healthy_images = load_and_resize_images(healthy_image_paths)\n",
    "\n",
    "# Labeling the dataset\n",
    "bleached_labels = ['bleached'] * len(bleached_images)\n",
    "healthy_labels = ['healthy'] * len(healthy_images)\n",
    "\n",
    "data = np.vstack((bleached_images, healthy_images))\n",
    "labels = np.array(bleached_labels + healthy_labels)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Split in training and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels_encoded, test_size=0.3, random_state=42)\n",
    "\n",
    "# Normalize the data\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# Data Augmentation using tf.data\n",
    "def preprocess_image(image, label):\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_flip_up_down(image)\n",
    "    image = tf.image.random_brightness(image, max_delta=0.1)\n",
    "    image = tf.image.random_contrast(image, lower=0.9, upper=1.1)\n",
    "    return image, label\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "train_dataset = train_dataset.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_dataset = train_dataset.batch(16).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "test_dataset = test_dataset.batch(16).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Transfer Learning with InceptionV3\n",
    "input_tensor = tf.keras.layers.Input(shape=(224, 224, 3))\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False, input_tensor=input_tensor)\n",
    "\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(1, activation='sigmoid', dtype='float32')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Unfreeze some layers of the base model\n",
    "for layer in base_model.layers[-50:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Compile the Model with a different learning rate\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.00001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Early Stopping and Learning Rate Reduction Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.00001)\n",
    "\n",
    "# Train the Model with Early Stopping and Data Augmentation\n",
    "history = model.fit(train_dataset,\n",
    "                    epochs=50,\n",
    "                    validation_data=test_dataset,\n",
    "                    callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "# Evaluate the Model\n",
    "loss, accuracy = model.evaluate(test_dataset)\n",
    "print(f'Validation accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Confusion Matrix\n",
    "y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Display Confusion Matrix using Seaborn\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Plot training and validation accuracy for each epoch\n",
    "train_accuracies = history.history['accuracy']\n",
    "val_accuracies = history.history['val_accuracy']\n",
    "epochs = range(1, len(train_accuracies) + 1)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(epochs, [acc * 100 for acc in train_accuracies], 'b', label='Training accuracy')\n",
    "plt.plot(epochs, [acc * 100 for acc in val_accuracies], 'g--', label='Validation accuracy')  # Changed to green dashed line\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Training and Validation Accuracy per Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
