{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d898b94f",
   "metadata": {},
   "source": [
    "# Healthy and Bleached Corals Image Classification using MobileNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05018c45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T20:22:48.629109Z",
     "start_time": "2024-12-10T20:22:46.116785Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.47.0-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.12/site-packages (2.5.1+cu124)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.12/site-packages (0.20.1+cu124)\n",
      "Requirement already satisfied: torchaudio in /opt/conda/lib/python3.12/site-packages (2.5.1+cu124)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.24.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.26.5-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.12/site-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.12/site-packages (from transformers) (4.67.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.12/site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.12/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /opt/conda/lib/python3.12/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /opt/conda/lib/python3.12/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /opt/conda/lib/python3.12/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /opt/conda/lib/python3.12/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /opt/conda/lib/python3.12/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.12/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /opt/conda/lib/python3.12/site-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch) (75.5.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.12/site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (2024.8.30)\n",
      "Downloading transformers-4.47.0-py3-none-any.whl (10.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m66.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.26.5-py3-none-any.whl (447 kB)\n",
      "Downloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (796 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m796.9/796.9 kB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (434 kB)\n",
      "Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, regex, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.26.5 regex-2024.11.6 safetensors-0.4.5 tokenizers-0.21.0 transformers-4.47.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7495b65b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T20:22:48.787155Z",
     "start_time": "2024-12-10T20:22:48.781612Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "from PIL import Image\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5e9531c8df60e3",
   "metadata": {},
   "source": [
    "A custom Dataset class is created to load the coral images from the file paths HEALTHY_IMAGES_DIR and BLEACHED_IMAGES_DIR.\n",
    "The subfolders contain images of healthy and bleached corals.\n",
    "\n",
    "The dataset used is from Kaggle: https://www.kaggle.com/datasets/vencerlanz09/healthy-and-bleached-corals-image-classification/data\n",
    "\n",
    "Dataset Details:\n",
    "+ Total images: 923\n",
    "+ Image categories: 2\n",
    "    + Healthy corals: 438 images\n",
    "    + Bleached corals: 485 images\n",
    "+ Image format: JPEG\n",
    "+ Image size: Maximum 300 px for either width or height, whichever is higher\n",
    "\n",
    "After loading the images the dta is splitted into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e03bfb6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T20:22:48.824021Z",
     "start_time": "2024-12-10T20:22:48.818054Z"
    }
   },
   "outputs": [],
   "source": [
    "class CoralDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(label, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bacc0da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T20:22:48.892012Z",
     "start_time": "2024-12-10T20:22:48.849717Z"
    }
   },
   "outputs": [],
   "source": [
    "HEALTHY_IMAGES_DIR = \"/data/healthy_corals\"\n",
    "healthy_image_paths = [os.path.join(HEALTHY_IMAGES_DIR, img) for img in os.listdir(HEALTHY_IMAGES_DIR) if os.path.isfile(os.path.join(HEALTHY_IMAGES_DIR, img))]\n",
    "BLEACHED_IMAGES_DIR = \"/data/bleached_corals\"\n",
    "bleached_image_paths = [os.path.join(BLEACHED_IMAGES_DIR, img) for img in os.listdir(BLEACHED_IMAGES_DIR) if os.path.isfile(os.path.join(BLEACHED_IMAGES_DIR, img))]\n",
    "\n",
    "image_paths = healthy_image_paths + bleached_image_paths\n",
    "labels = [0] * len(healthy_image_paths) + [1] * len(bleached_image_paths)\n",
    "\n",
    "# Split the data into train and validation sets\n",
    "train_paths, val_paths, train_labels, val_labels = train_test_split(image_paths, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d49f05f79c19ab0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T20:24:30.303702Z",
     "start_time": "2024-12-10T20:22:48.920498Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f16425eb331a4690a58ffaf95fb13b56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/69.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91acb48a3be049ac90b30b200b374c7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/14.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Running Configuration 1 ###\n",
      "\n",
      "Epoch [1/10], Loss: 0.5506, Val Loss: 0.5151, Val Accuracy: 74.59%\n",
      "Epoch [2/10], Loss: 0.3213, Val Loss: 0.4903, Val Accuracy: 75.14%\n",
      "Epoch [3/10], Loss: 0.1770, Val Loss: 0.5051, Val Accuracy: 73.51%\n",
      "Epoch [4/10], Loss: 0.0896, Val Loss: 0.6063, Val Accuracy: 76.76%\n",
      "Epoch [5/10], Loss: 0.0553, Val Loss: 0.6672, Val Accuracy: 74.05%\n",
      "Epoch [6/10], Loss: 0.0345, Val Loss: 0.7298, Val Accuracy: 74.05%\n",
      "Epoch [7/10], Loss: 0.0264, Val Loss: 0.6955, Val Accuracy: 72.97%\n",
      "Epoch [8/10], Loss: 0.0228, Val Loss: 0.8705, Val Accuracy: 71.89%\n",
      "Epoch [9/10], Loss: 0.0280, Val Loss: 0.6872, Val Accuracy: 74.59%\n",
      "Epoch [10/10], Loss: 0.0185, Val Loss: 0.9137, Val Accuracy: 68.11%\n",
      "\n",
      "### Running Configuration 2 ###\n",
      "\n",
      "Epoch [1/15], Loss: 0.0713, Val Loss: 0.8659, Val Accuracy: 70.27%\n",
      "Epoch [2/15], Loss: 0.0423, Val Loss: 0.6531, Val Accuracy: 76.22%\n",
      "Epoch [3/15], Loss: 0.0661, Val Loss: 1.0217, Val Accuracy: 70.27%\n",
      "Epoch [4/15], Loss: 0.0273, Val Loss: 0.9129, Val Accuracy: 70.81%\n",
      "Epoch [5/15], Loss: 0.0217, Val Loss: 0.7017, Val Accuracy: 76.76%\n",
      "Epoch [6/15], Loss: 0.0224, Val Loss: 0.9149, Val Accuracy: 68.65%\n",
      "Epoch [7/15], Loss: 0.0166, Val Loss: 0.6482, Val Accuracy: 73.51%\n",
      "Epoch [8/15], Loss: 0.0268, Val Loss: 1.2660, Val Accuracy: 65.41%\n",
      "Epoch [9/15], Loss: 0.0237, Val Loss: 1.4758, Val Accuracy: 70.81%\n",
      "Epoch [10/15], Loss: 0.0158, Val Loss: 0.8385, Val Accuracy: 70.27%\n",
      "Epoch [11/15], Loss: 0.0135, Val Loss: 0.6577, Val Accuracy: 80.00%\n",
      "Epoch [12/15], Loss: 0.0136, Val Loss: 0.7065, Val Accuracy: 68.65%\n",
      "Epoch [13/15], Loss: 0.0161, Val Loss: 0.6614, Val Accuracy: 78.38%\n",
      "Epoch [14/15], Loss: 0.0222, Val Loss: 0.7398, Val Accuracy: 74.59%\n",
      "Epoch [15/15], Loss: 0.0117, Val Loss: 0.6036, Val Accuracy: 73.51%\n",
      "\n",
      "### Running Configuration 3 ###\n",
      "\n",
      "Epoch [1/10], Loss: 0.2139, Val Loss: 0.7788, Val Accuracy: 73.51%\n",
      "Epoch [2/10], Loss: 0.1603, Val Loss: 0.8283, Val Accuracy: 78.92%\n",
      "Epoch [3/10], Loss: 0.0899, Val Loss: 0.8389, Val Accuracy: 75.14%\n",
      "Epoch [4/10], Loss: 0.0793, Val Loss: 1.0017, Val Accuracy: 71.89%\n",
      "Epoch [5/10], Loss: 0.0698, Val Loss: 0.8952, Val Accuracy: 76.22%\n",
      "Epoch [6/10], Loss: 0.1123, Val Loss: 1.2296, Val Accuracy: 72.43%\n",
      "Epoch [7/10], Loss: 0.1063, Val Loss: 1.1486, Val Accuracy: 75.14%\n",
      "Epoch [8/10], Loss: 0.0685, Val Loss: 0.9054, Val Accuracy: 77.84%\n",
      "Epoch [9/10], Loss: 0.0566, Val Loss: 0.9176, Val Accuracy: 77.30%\n",
      "Epoch [10/10], Loss: 0.0755, Val Loss: 0.7350, Val Accuracy: 75.68%\n",
      "\n",
      "### Running Configuration 4 ###\n",
      "\n",
      "Epoch [1/20], Loss: 0.0446, Val Loss: 1.1173, Val Accuracy: 75.14%\n",
      "Epoch [2/20], Loss: 0.0397, Val Loss: 1.1090, Val Accuracy: 77.30%\n",
      "Epoch [3/20], Loss: 0.0283, Val Loss: 1.0854, Val Accuracy: 71.89%\n",
      "Epoch [4/20], Loss: 0.0155, Val Loss: 1.2158, Val Accuracy: 74.59%\n",
      "Epoch [5/20], Loss: 0.0235, Val Loss: 0.9236, Val Accuracy: 76.76%\n",
      "Epoch [6/20], Loss: 0.0197, Val Loss: 1.1130, Val Accuracy: 74.05%\n",
      "Epoch [7/20], Loss: 0.0176, Val Loss: 1.0895, Val Accuracy: 75.14%\n",
      "Epoch [8/20], Loss: 0.0168, Val Loss: 1.1666, Val Accuracy: 71.89%\n",
      "Epoch [9/20], Loss: 0.0131, Val Loss: 0.9143, Val Accuracy: 77.84%\n",
      "Epoch [10/20], Loss: 0.0149, Val Loss: 1.3527, Val Accuracy: 73.51%\n",
      "Epoch [11/20], Loss: 0.0128, Val Loss: 1.1904, Val Accuracy: 76.22%\n",
      "Epoch [12/20], Loss: 0.0150, Val Loss: 0.9848, Val Accuracy: 77.30%\n",
      "Epoch [13/20], Loss: 0.0120, Val Loss: 1.7478, Val Accuracy: 72.43%\n",
      "Epoch [14/20], Loss: 0.0105, Val Loss: 1.6161, Val Accuracy: 79.46%\n",
      "Epoch [15/20], Loss: 0.0102, Val Loss: 1.2696, Val Accuracy: 73.51%\n",
      "Epoch [16/20], Loss: 0.0124, Val Loss: 1.6519, Val Accuracy: 76.76%\n",
      "Epoch [17/20], Loss: 0.0121, Val Loss: 0.9844, Val Accuracy: 79.46%\n",
      "Epoch [18/20], Loss: 0.0112, Val Loss: 2.4219, Val Accuracy: 73.51%\n",
      "Epoch [19/20], Loss: 0.0109, Val Loss: 1.6329, Val Accuracy: 76.22%\n",
      "Epoch [20/20], Loss: 0.0136, Val Loss: 1.0943, Val Accuracy: 77.84%\n"
     ]
    }
   ],
   "source": [
    "# Modify batch size, learning rate, augmentation, and epochs dynamically\n",
    "\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "runs = [\n",
    "    {\"batch_size\": 40, \"lr\": 0.0001, \"augmentation\": \"default\", \"epochs\": 10},\n",
    "    {\"batch_size\": 60, \"lr\": 0.00005, \"augmentation\": \"horizontal_flip\", \"epochs\": 15},\n",
    "    {\"batch_size\": 20, \"lr\": 0.0002, \"augmentation\": \"rotation\", \"epochs\": 10},\n",
    "    {\"batch_size\": 40, \"lr\": 0.0001, \"augmentation\": \"color_jitter\", \"epochs\": 20},\n",
    "]\n",
    "\n",
    "# Load the pretrained MobileNet model\n",
    "model = AutoModelForImageClassification.from_pretrained(\"google/mobilenet_v2_1.0_224\")\n",
    "\n",
    "# Replace the classifier layer to output a single value for binary classification\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Linear(model.classifier.in_features, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "# Set device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Set up the optimizer and loss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "for i, run in enumerate(runs):\n",
    "    print(f\"\\n### Running Configuration {i + 1} ###\\n\")\n",
    "    \n",
    "    # Set data augmentation\n",
    "    if run[\"augmentation\"] == \"default\":\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=mean, std=std),\n",
    "        ])\n",
    "    elif run[\"augmentation\"] == \"horizontal_flip\":\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=mean, std=std),\n",
    "        ])\n",
    "    elif run[\"augmentation\"] == \"rotation\":\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.RandomRotation(15),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=mean, std=std),\n",
    "        ])\n",
    "    elif run[\"augmentation\"] == \"color_jitter\":\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=mean, std=std),\n",
    "        ])\n",
    "    \n",
    "    # Update dataset and dataloaders\n",
    "    train_dataset = CoralDataset(train_paths, train_labels, transform=transform)\n",
    "    val_dataset = CoralDataset(val_paths, val_labels, transform=transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=run[\"batch_size\"], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=run[\"batch_size\"], shuffle=False)\n",
    "    \n",
    "    # Update optimizer learning rate\n",
    "    optimizer = optim.Adam(model.parameters(), lr=run[\"lr\"])\n",
    "    \n",
    "    # Train the model\n",
    "    for epoch in range(run[\"epochs\"]):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images).logits.squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images).logits.squeeze()\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                predicted = (outputs > 0.5).float()\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_accuracy = 100 * correct / total\n",
    "        print(f\"Epoch [{epoch+1}/{run['epochs']}], Loss: {running_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(val_loader):.4f}, Val Accuracy: {val_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5b6c8103d48f80",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **Result Table After Each Run**\n",
    "\n",
    "| **Run** | **Batch Size** | **Learning Rate** | **Augmentation**                | **Epochs** | **Final Train Loss** | **Final Val Loss** | **Final Val Accuracy** |\n",
    "|---------|----------------|-------------------|----------------------------------|------------|-----------------------|---------------------|-------------------------|\n",
    "| 1       | 40             | 0.0001           | Resize, Normalize               | 10         |                       |                     |                         |\n",
    "| 2       | 60             | 0.00005          | Horizontal Flip                 | 15         |                       |                     |                         |\n",
    "| 3       | 20             | 0.0002           | Rotation (15°)                  | 10         |                       |                     |                         |\n",
    "| 4       | 40             | 0.0001           | Horizontal Flip + Color Jitter  | 20         |                       |                     |                         |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
